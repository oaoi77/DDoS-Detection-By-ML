{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIC dataset - Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"yellow\">**User**</font> <br>\n",
    "Number of users: 25\n",
    "User Protocols: HTTP, HTTPS, FTP , SSH and email\n",
    "\n",
    "<font color = \"red\">**Attacker**</font> <br>\n",
    "**First day:**\n",
    "- LDAP\n",
    "- MSSQL\n",
    "- NetBIOS\n",
    "- Portmap\n",
    "- Syn\n",
    "- UDP\n",
    "- UDPLag\n",
    "\n",
    "**Second day:**\n",
    "- NTP\n",
    "- DNS\n",
    "- LDAP\n",
    "- MSSQL\n",
    "- NetBIOS\n",
    "- SNMP\n",
    "- SSDP\n",
    "- UDP\n",
    "- UDPLag\n",
    "- WebDDoS\n",
    "- SYN\n",
    "- TFTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning: is a game-changing concept in machine learning. It's about leveraging knowledge from one task to improve performance of another. It accelerates learning, enhances generalization and adapts model to new challenges by building on existing expertise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Information\n",
    "- Flow ID: Unique identifier for a network communication flow.\n",
    "- Source IP: IP address of the device that initiated the communication.\n",
    "- Source Port: Port number used by the source device.\n",
    "- Destination IP: IP address of the target device.\n",
    "- Destination Port: Port number on the target device.\n",
    "- Protocol: Network protocol used (e.g., TCP, UDP).\n",
    "- Timestamp: Time when the flow or packet was recorded.\n",
    "- Flow Duration: Total time (in microseconds) for the flow's duration.\n",
    "- Total Fwd Packets: Total number of packets sent from source to destination in the flow.\n",
    "- Total Backward Packets: Total number of packets sent from destination to source in the flow.\n",
    "- Total Length of Fwd Packets: Total size (in bytes) of all forward packets in the flow.\n",
    "- Total Length of Bwd Packets: Total size (in bytes) of all backward packets in the flow.\n",
    "- Fwd Packet Length Max: Maximum size (in bytes) of packets sent forward in the flow.\n",
    "- Fwd Packet Length Min: Minimum size (in bytes) of packets sent forward in the flow.\n",
    "- Fwd Packet Length Mean: Average size (in bytes) of packets sent forward in the flow.\n",
    "- Fwd Packet Length Std: Standard deviation of the sizes of forward packets in the flow.\n",
    "- Bwd Packet Length Max: Maximum size (in bytes) of packets sent backward in the flow.\n",
    "- Bwd Packet Length Min: Minimum size (in bytes) of packets sent backward in the flow.\n",
    "- Bwd Packet Length Mean: Average size (in bytes) of packets sent backward in the flow.\n",
    "- Bwd Packet Length Std: Standard deviation of the sizes of backward packets in the flow.\n",
    "- Flow Bytes/s: Average number of bytes transferred per second in the flow.\n",
    "- Flow Packets/s: Average number of packets transferred per second in the flow.\n",
    "- Flow IAT Mean: Average inter-arrival time (IAT) between packets in the flow.\n",
    "- Flow IAT Std: Standard deviation of the inter-arrival times (IAT) between packets in the flow.\n",
    "- Flow IAT Max: Maximum inter-arrival time (IAT) between packets in the flow.\n",
    "- Flow IAT Min: Minimum inter-arrival time (IAT) between packets in the flow.\n",
    "- Fwd IAT Total: Total inter-arrival time (IAT) for packets sent forward in the flow.\n",
    "- Fwd IAT Mean: Average inter-arrival time (IAT) of forward packets in the flow.\n",
    "- Fwd IAT Std: Standard deviation of the inter-arrival times (IAT) for forward packets in the flow.\n",
    "- Fwd IAT Max: Maximum inter-arrival time (IAT) between forward packets in the flow.\n",
    "- Fwd IAT Min: Minimum inter-arrival time (IAT) between forward packets in the flow.\n",
    "- Bwd IAT Total: Total inter-arrival time (IAT) for packets sent backward in the flow.\n",
    "- Bwd IAT Mean: Average inter-arrival time (IAT) of backward packets in the flow.\n",
    "- Bwd IAT Std: Standard deviation of the inter-arrival times (IAT) for backward packets in the flow.\n",
    "- Bwd IAT Max: Maximum inter-arrival time (IAT) between backward packets in the flow.\n",
    "- Bwd IAT Min: Minimum inter-arrival time (IAT) between backward packets in the flow.\n",
    "- Fwd PSH Flags: The number of forward packets with the PSH (Push) flag set in the TCP header. The PSH flag tells the receiver to push the data to the application layer immediately.\n",
    "- Bwd PSH Flags: The number of backward packets with the PSH (Push) flag set in the TCP header.\n",
    "- Fwd URG Flags: The number of forward packets with the URG (Urgent) flag set in the TCP header.\n",
    "- Bwd URG Flags: The number of backward packets with the URG (Urgent) flag set in the TCP header.\n",
    "- Fwd Header Length: The total length (in bytes) of headers for the forward packets in the flow.\n",
    "- Bwd Header Length: The total length (in bytes) of headers for the backward packets in the flow.\n",
    "- Fwd Packets/s: Average number of forward packets transferred per second in the flow.\n",
    "- Bwd Packets/s: Average number of backward packets transferred per second in the flow.\n",
    "- Min Packet Length: Minimum size (in bytes) of any packet in the flow.\n",
    "- Max Packet Length: Maximum size (in bytes) of any packet in the flow.\n",
    "- Packet Length Mean: Average size (in bytes) of all packets in the flow.\n",
    "- Packet Length Std: Standard deviation of the packet sizes in the flow.\n",
    "- Packet Length Variance: Variance of the packet sizes in the flow, indicating the spread of packet lengths.\n",
    "- FIN Flag Count: The number of packets with the FIN (Finish) flag set, indicating the end of a connection.\n",
    "- SYN Flag Count: The number of packets with the SYN (Synchronize) flag set, used in the TCP handshake to establish a connection.\n",
    "- RST Flag Count: The number of packets with the RST (Reset) flag set, used to reset a TCP connection.\n",
    "- PSH Flag Count: The number of packets with the PSH (Push) flag set in the TCP header, signaling the receiver to pass the data to the application layer immediately.\n",
    "- ACK Flag Count: The number of packets with the ACK (Acknowledgment) flag set, indicating that the packet is acknowledging receipt of data.\n",
    "- URG Flag Count: The number of packets with the URG (Urgent) flag set in the TCP header, indicating that the packet contains urgent data.\n",
    "- CWE Flag Count: The number of packets with the CWE (Congestion Window Reduced) flag set, indicating that congestion control is in effect.\n",
    "- ECE Flag Count: The number of packets with the ECE (ECN Echo) flag set, used for explicit congestion notification in TCP.\n",
    "- Down/Up Ratio: The ratio of backward (downstream) packets to forward (upstream) packets in the flow.\n",
    "- Average Packet Size: The average size (in bytes) of all packets in the flow.\n",
    "- Avg Fwd Segment Size: The average size (in bytes) of the forward TCP segments in the flow.\n",
    "- Avg Bwd Segment Size: The average size (in bytes) of backward TCP segments in the flow.\n",
    "- Fwd Header Length.1: The total length (in bytes) of the headers for the forward packets, likely a repetition or additional field in the dataset.\n",
    "- Fwd Avg Bytes/Bulk: The average number of bytes per bulk transfer for the forward packets in the flow.\n",
    "- Fwd Avg Packets/Bulk: The average number of packets per bulk transfer for the forward packets in the flow.\n",
    "- Fwd Avg Bulk Rate: The average rate of bulk data transfer (in bytes per second) for the forward packets.\n",
    "- Bwd Avg Bytes/Bulk: The average number of bytes per bulk transfer for the backward packets in the flow.\n",
    "- Bwd Avg Packets/Bulk: The average number of packets per bulk transfer for the backward packets in the flow.\n",
    "- Bwd Avg Bulk Rate: The average rate of bulk data transfer (in bytes per second) for the backward packets.\n",
    "- Subflow Fwd Packets: The number of forward packets in subflows of a connection (in a multi-flow scenario).\n",
    "- Subflow Fwd Packets: The number of forward packets in subflows of a connection, usually in a multi-path or segmented flow.\n",
    "- Subflow Fwd Bytes: The total number of bytes in the forward packets of subflows in a connection.\n",
    "- Subflow Bwd Packets: The number of backward packets in subflows of a connection.\n",
    "- Subflow Bwd Bytes: The total number of bytes in the backward packets of subflows in a connection.\n",
    "- Init_Win_bytes_forward: The initial window size (in bytes) for the forward direction in the TCP connection.\n",
    "- Init_Win_bytes_backward: The initial window size (in bytes) for the backward direction in the TCP connection.\n",
    "- act_data_pkt_fwd: The number of active data packets in the forward direction (non-control packets) in the flow.\n",
    "- min_seg_size_forward: The minimum segment size (in bytes) for the forward packets in the flow.\n",
    "- Active Mean: The average activity level, likely based on packet transmission rate or frequency of packet activity.\n",
    "- Active Std: The standard deviation of the active packet transmission rate, representing variability in packet activity.\n",
    "- Active Max: The maximum rate of active packet transmission observed in the flow.\n",
    "- Active Min: The minimum rate of active packet transmission observed in the flow.\n",
    "- Idle Mean: The average idle time (when no packets are transmitted) in the flow.\n",
    "- Idle Std: The standard deviation of idle times, indicating the variability in the periods of inactivity.\n",
    "- Idle Max: The maximum idle time between packet transmissions in the flow.\n",
    "- Idle Min: The minimum idle time between packet transmissions in the flow.\n",
    "- SimillarHTTP: A feature indicating whether the flow has characteristics similar to HTTP traffic, potentially used for anomaly detection.\n",
    "- Inbound: A flag or indicator specifying whether the flow is inbound (incoming) or outbound (outgoing) traffic.\n",
    "- Label: The classification label for the flow, used for attack detection or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"color: yellow\">Model Test </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_extracted_flow_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_train \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_extracted_flow_features.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m df_test \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_extracted_flow_features.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/media/V_storage/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m/media/V_storage/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m/media/V_storage/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[0;32m/media/V_storage/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m   1881\u001B[0m     f,\n\u001B[1;32m   1882\u001B[0m     mode,\n\u001B[1;32m   1883\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1884\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1885\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m   1886\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[1;32m   1887\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1888\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1889\u001B[0m )\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/media/V_storage/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    874\u001B[0m             handle,\n\u001B[1;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    876\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    877\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    879\u001B[0m         )\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'train_extracted_flow_features.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train_extracted_flow_features.csv\")\n",
    "df_test = pd.read_csv(\"test_extracted_flow_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['benign', 'BENIGN', 'MSSQL', 'UDP', 'attacker'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "UDP         28022\n",
       "BENIGN       1122\n",
       "benign        631\n",
       "MSSQL         182\n",
       "attacker        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['benign', 'BENIGN', 'DrDoS_UDP'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "DrDoS_UDP    19413\n",
       "BENIGN         994\n",
       "benign         327\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color: orange\"> Logistic Regression </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Accuracy: 0.93\n",
      "Confusion Matrix:\n",
      "[[  898    96     0]\n",
      " [    2   325     0]\n",
      " [19411     2     0]]\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.04421249569198956, 'recall': 0.903420523138833, 'f1-score': 0.08429946022060548, 'support': 994.0}, '4': {'precision': 0.7683215130023641, 'recall': 0.9938837920489296, 'f1-score': 0.8666666666666667, 'support': 327.0}, '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 19413.0}, 'accuracy': 0.058985241632101865, 'macro avg': {'precision': 0.27084466956478453, 'recall': 0.6324347717292542, 'f1-score': 0.3169887089624241, 'support': 20734.0}, 'weighted avg': {'precision': 0.0142369227100227, 'recall': 0.058985241632101865, 'f1-score': 0.017709735866657753, 'support': 20734.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from research.External.CustomEncoder import CustomLabelEncoder\n",
    "\n",
    "# Encoding the object value\n",
    "columnNeedEncode = ['Flow ID', 'Src IP', 'Dst IP', 'label']\n",
    "encoded_map = {}\n",
    "\n",
    "for col in columnNeedEncode:\n",
    "    encoder = CustomLabelEncoder()\n",
    "    df_train[col] = encoder.fit_transform(df_train[col])\n",
    "    df_test[col] = encoder.transform(df_test[col])\n",
    "\n",
    "    # encoded_map[f\"{col}_map\"] = {label: value for value, label in enumerate(encoder.encoder.classes_)}\n",
    "    encoded_map[f\"{col}_map\"] = encoder.show_mapping()\n",
    "\n",
    "\n",
    "# Split data into X and y\n",
    "X_train = df_train.drop(columns=['label'])  \n",
    "y_train = df_train['label']\n",
    "X_test = df_test.drop(columns=['label'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict= True)\n",
    "\n",
    "# Extract valid classes (support > 0)\n",
    "valid_classes = [cls for cls, metrics in report.items() if isinstance(metrics, dict) and metrics[\"support\"] > 0]\n",
    "classes_toRemove = ['5', 'micro avg', 'macro avg', 'weighted avg']\n",
    "valid_classes = [int(cls) for cls in valid_classes if cls not in classes_toRemove]\n",
    "\n",
    "# Filter valid samples from y_test and y_pred\n",
    "filtered_y_test = [y for y in y_test if y in valid_classes]\n",
    "filtered_y_pred = [y for i, y in enumerate(y_pred) if y_test[i] in valid_classes]\n",
    "\n",
    "# Calculate accuracy for valid classes only\n",
    "filtered_accuracy = accuracy_score(filtered_y_test, filtered_y_pred)\n",
    "print(f\"Filtered Accuracy: {filtered_accuracy:.2f}\")\n",
    "\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'BENIGN',\n",
       " 1: 'LDAP',\n",
       " 2: 'NetBIOS',\n",
       " 3: 'attacker',\n",
       " 4: 'benign',\n",
       " 5: 'DrDoS_LDAP'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_map['label_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n",
      "1321\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_y_test))\n",
    "print(len(filtered_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "5    14450\n",
       "0      572\n",
       "4      198\n",
       "3       29\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "2    14468\n",
       "0      502\n",
       "4      217\n",
       "1       35\n",
       "3       27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color: orange\"> Random Forest </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[  994     0     0     0     0]\n",
      " [    0     0     0     0     0]\n",
      " [    0     0     0     0     0]\n",
      " [    1     0     0   326     0]\n",
      " [ 6423    92 12898     0     0]]\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.133998382313292, 'recall': 1.0, 'f1-score': 0.23632905373276272, 'support': 994.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0}, '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0}, '4': {'precision': 1.0, 'recall': 0.9969418960244648, 'f1-score': 0.998468606431853, 'support': 327.0}, '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 19413.0}, 'accuracy': 0.06366354779589081, 'macro avg': {'precision': 0.2267996764626584, 'recall': 0.399388379204893, 'f1-score': 0.24695953203292315, 'support': 20734.0}, 'weighted avg': {'precision': 0.022195157327067245, 'recall': 0.06366354779589081, 'f1-score': 0.02707679722743234, 'support': 20734.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from research.External.CustomEncoder import CustomLabelEncoder\n",
    "\n",
    "# Encoding the object value\n",
    "columnNeedEncode = ['Flow ID', 'Src IP', 'Dst IP', 'label']\n",
    "encoded_map = {}\n",
    "\n",
    "for col in columnNeedEncode:\n",
    "    encoder = CustomLabelEncoder()\n",
    "    df_train[col] = encoder.fit_transform(df_train[col])\n",
    "    df_test[col] = encoder.transform(df_test[col])\n",
    "\n",
    "    # Save the encoding map\n",
    "    encoded_map[f\"{col}_map\"] = encoder.show_mapping()\n",
    "\n",
    "# Split data into X and y\n",
    "X_train = df_train.drop(columns=['label'])  \n",
    "y_train = df_train['label']\n",
    "X_test = df_test.drop(columns=['label'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Extract valid classes (support > 0)\n",
    "valid_classes = [cls for cls, metrics in report.items() if isinstance(metrics, dict) and metrics[\"support\"] > 0]\n",
    "classes_toRemove = ['5', 'micro avg', 'macro avg', 'weighted avg']\n",
    "valid_classes = [int(cls) for cls in valid_classes if cls not in classes_toRemove]\n",
    "\n",
    "# Filter valid samples from y_test and y_pred\n",
    "filtered_y_test = [y for y in y_test if y in valid_classes]\n",
    "filtered_y_pred = [y for i, y in enumerate(y_pred) if y_test[i] in valid_classes]\n",
    "\n",
    "# Calculate accuracy for valid classes only\n",
    "filtered_accuracy = accuracy_score(filtered_y_test, filtered_y_pred)\n",
    "print(f\"Filtered Accuracy: {filtered_accuracy:.2f}\")\n",
    "\n",
    "# Print Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('0', {'precision': 0.03807748635334842, 'recall': 1.0, 'f1-score': 0.07336154931383866, 'support': 572.0}), ('3', {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 29.0}), ('4', {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 198.0}), ('5', {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 14450.0}), ('accuracy', 0.05239687848383501), ('macro avg', {'precision': 0.509519371588337, 'recall': 0.75, 'f1-score': 0.5183403873284597, 'support': 15249.0}), ('weighted avg', {'precision': 0.016314533555912867, 'recall': 0.05239687848383501, 'f1-score': 0.017638061919307214, 'support': 15249.0})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15249\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(y_pred)\n",
    "a.value_counts()\n",
    "a.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15249"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()\n",
    "y_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n",
      "799\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_y_test))\n",
    "print(len(filtered_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color: orange\"> SVM </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Accuracy: 0.65\n",
      "Confusion Matrix:\n",
      "[[  465   107     0     0     0]\n",
      " [    0     0     0     0     0]\n",
      " [    0    29     0     0     0]\n",
      " [  109    34     0    55     0]\n",
      " [    6 14444     0     0     0]]\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.8017241379310345, 'recall': 0.8129370629370629, 'f1-score': 0.8072916666666666, 'support': 572.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0}, '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, '4': {'precision': 1.0, 'recall': 0.2777777777777778, 'f1-score': 0.43478260869565216, 'support': 198.0}, '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 14450.0}, 'accuracy': 0.03410059676044331, 'macro avg': {'precision': 0.3603448275862069, 'recall': 0.21814296814296813, 'f1-score': 0.24841485507246377, 'support': 15249.0}, 'weighted avg': {'precision': 0.043057656692015986, 'recall': 0.03410059676044331, 'f1-score': 0.03592745687291445, 'support': 15249.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/media/V_storage/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from research.External.CustomEncoder import CustomLabelEncoder\n",
    "\n",
    "# Encoding the object value\n",
    "columnNeedEncode = ['Flow ID', 'Src IP', 'Dst IP', 'label']\n",
    "encoded_map = {}\n",
    "\n",
    "for col in columnNeedEncode:\n",
    "    encoder = CustomLabelEncoder()\n",
    "    df_train[col] = encoder.fit_transform(df_train[col])\n",
    "    df_test[col] = encoder.transform(df_test[col])\n",
    "\n",
    "    # Save the encoding map\n",
    "    encoded_map[f\"{col}_map\"] = encoder.show_mapping()\n",
    "\n",
    "# Split data into X and y\n",
    "X_train = df_train.drop(columns=['label'])  \n",
    "y_train = df_train['label']\n",
    "X_test = df_test.drop(columns=['label'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)  # Customize hyperparameters if needed\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Extract valid classes (support > 0)\n",
    "valid_classes = [cls for cls, metrics in report.items() if isinstance(metrics, dict) and metrics[\"support\"] > 0]\n",
    "classes_toRemove = ['5', 'micro avg', 'macro avg', 'weighted avg']\n",
    "valid_classes = [int(cls) for cls in valid_classes if cls not in classes_toRemove]\n",
    "\n",
    "# Filter valid samples from y_test and y_pred\n",
    "filtered_y_test = [y for y in y_test if y in valid_classes]\n",
    "filtered_y_pred = [y for i, y in enumerate(y_pred) if y_test[i] in valid_classes]\n",
    "\n",
    "# Calculate accuracy for valid classes only\n",
    "filtered_accuracy = accuracy_score(filtered_y_test, filtered_y_pred)\n",
    "print(f\"Filtered Accuracy: {filtered_accuracy:.2f}\")\n",
    "\n",
    "# Print Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n",
      "799\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_y_test))\n",
    "print(len(filtered_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
